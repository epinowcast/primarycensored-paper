---
title: "Modelling delays with primary Event Censored Distributions"
author:
  - name: Samuel P. C. Brand
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: true
  - name: Kelly Charniga
    affiliations:
      - name: Institut Pasteur, France
    corresponding: false
  - name: Sang Woo Park
    affiliations:
      - name: Department of Ecology and Evolution, University of Chicago, Chicago, Illinois, United States of America
    corresponding: false
  - name: James Mba Azam
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Adam Howes
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: false
  - name: Carl Pearson
    affiliations:
      - name: Affiliation
    corresponding: false
  - name: Sebastian Funk
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Sam Abbott
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
    email: sam.abbott@lshtm.ac.uk
date: today
format:
  html:
    toc: false
    number-sections: false
bibliography: reference.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/plos.csl
---

## Abstract

Delay distributions are essential for understanding disease dynamics but are frequently biased by censoring and truncation. Current approaches for handling double interval censored data either discard observations, rely on discretisation, treat unobserved primary events as individual parameters, creating computational barriers for large datasets, or cannot readily be extended to account for truncation. We propose a statistically rigorous yet computationally efficient approach that marginalises over latent primary event times rather than estimating them individually. We first develop a general numerical solution and then derive analytical solutions for commonly used distributions including gamma, lognormal and Weibull distributions. We implement these methods in the open-source primarycensored R package with R and Stan interfaces and an extension for the fitdistrplus package. We validate our approach using simulated data. We then apply our method to case data from the 2014-2016 Sierra Leone Ebola epidemic, comparing to the current best practice latent parameter approach. Our method maintains statistical integrity whilst improving scalability for large-scale surveillance datasets, enabling accurate delay distribution estimation where previous approaches become computationally intractable. The standardised framework and accompanying software tools facilitate integration with existing epidemiological models.

## Author summary

## Introduction

Time-to-event distributions are essential in epidemiology, describing delays between events like infection and symptom onset. These inform disease natural history, enable nowcasting, and support epidemic reconstruction. However, real-world surveillance data present challenges due to censoring and truncation, which can significantly bias estimates if not properly addressed [@charniga2024best]. Inaccurate delay distributions directly impact critical response decisions during outbreaks, from resource allocation to intervention timing, potentially leading to mischaracterisation of epidemic dynamics and delayed public health actions [@Park2024.01.12.24301247].

A key challenge in epidemiological data is interval censoring, where event times are known only within specific intervals. Primary event censoring affects the initial event time (e.g., infection), secondary event censoring affects the end event time (e.g., symptom onset), and double interval censoring occurs when both are present [@charniga2024best; @Reich2009-aa]. These issues are compounded by right truncation, where events with longer delays are systematically missing from recent data because the secondary event hasn't yet occurred or been observed [@Park2024.01.12.24301247]. Each form of censoring introduces distinct biases that must be addressed for accurate estimation [@law1992effects].

Several approaches attempt to address these challenges, but each has limitations. Simple methods include discarding censored observations (infeasible when all data are censored) [@little2019statistical], discretisation (treating delays as occurring in fixed intervals), or fixed imputation (assuming events occur at interval midpoints). Park et al. found that treating unobserved primary event times as latent parameters proposed by Ward et al. improved upon fixed imputation [@Park2024.01.12.24301247; @Ward2022-wo], but this introduces N additional parameters when analysing N observations, limiting scalability to large datasets. Charniga et al. highlighted a persistent gap in computationally efficient methods for double interval censored data that maintain statistical rigour while scaling to large surveillance datasets [@charniga2024best].

In this paper, we aim to establish a standard approach for epidemiological delay distribution estimation that is statistically rigourous and computationally efficient. We extend Ward et al.'s method by marginalising over latent primary event times rather than treating them as individual parameters, maintaining statistical integrity while substantially improving scalability for large datasets. We first show that when the primary event time is known, the observed delay PMF can be calculated as the increase in cumulative distribution function (CDF) of the true delay distribution over the secondary event censoring window. We then show that marginalising over the primary event time maintains this relationship, albeit with an adjusted CDF that accounts for the primary event censoring. We provide a general numerically solvable solution for the adjusted CDF and then derive analytical solutions for common parametric distributions, including gamma, lognormal, and Weibull. We implement these methods in the open-source R package 'primarycensored' [@abbottprimarycensored], with both R functions and Stan extensions that integrate with existing tools like fitdistrplus and epidemiological modelling packages [@fitdistrplus; @stan; @abbottepinow2; @Cori2013]. We then evaluate our method using both simulated data and real-world case studies for a range of distributions. We first compare numerical accuracy and run time with the Ward et al. approach for recovering PMFs and then demonstrate parameter recovery versus a naive approach. Our method makes accurate delay distribution estimation feasible for large-scale surveillance datasets where previous approaches become computationally intractable. Our software tools enable using these approaches as part of more complex epidemiological models.

## Methods

### Statistical framework

#### Problem statement

For an individual, a primary event (e.g. infection) occurs at time $P_u$, followed by a secondary event (e.g. symptom onset) at time $S$. The true delay between these events is $T = S - P_u$, which follows some density $f_{T; \theta}(t)$ which has distribution parameters $\theta$ and a cumulative distribution function $F_{T; \theta}(t)$. 

In practice, however, neither $P_u$ nor $S$ is typically observed. Instead, we observe that the primary event occurred within a window $[t_P, t_P + w_P]$, where $w_P$ is the width of the primary censoring interval (often one day in surveillance data). Similarly, the secondary event is observed to occur within $[t_S, t_S + w_S]$, where $w_S$ is the width of the secondary censoring interval.

The secondary event time is naturally bounded below by $P_u$, whereas $P_u$ is in principle unconstrained. In the following analysis we also consider the censored primary event time,

$$P = P_u \mid P_u \in [t_P, t_P + w_P].$$ {#eq-defP}

The censored primary event time $P$ is bounded below by the start of the primary window, which will be useful in the derivations below. 
The observed censored delay time, $T_c$, is measured from the start of the primary window to the start of the secondary window: $T_c = t_S - t_P$.

The statistical problem is two-fold: first, given some delay distribution $T \sim f_{T;\theta}$ what is the corresponding probability mass distribution (PMF) for the censored delays $\Pr(T_c \mid \theta)$? Second, is the inverse problem, given a collection of $N$ data items, each giving linked information about primary and secondary event pairs (e.g. a linelist of cases), what is the log-likelihood for the parameters of the underlying true delay distribution $\theta$?    

#### Generative process and simulation for delay data

With non-informative censoring (where the observation process is independent of the actual event times), we model the distribution of the censored primary event time as:

$$f_P(p) = f_{P_u \mid P_u \in [t_P, t_P + w_P]}(p) = \frac{f_{P_u}(p)}{F_{P_u}(t_P + w_P) - F_{P_u}(t_P)}, \quad p \in [t_P, t_P + w_P]$$

In the simplest case, primary events occur uniformly within their censoring windows, but in real epidemiological situations, they often follow non-uniform patterns. During epidemic growth, for example, events are more likely to occur near the end of their censoring windows, while during decline, they are more likely to occur near the beginning. This can be modeled using an exponentially tilted distribution:

$$f_P(p) \propto \exp(r p) \mathbb{1}_{[t_P, t_P + w_P]}(p)$$ {#eq-condP}

where $r$ controls the growth/decline rate.

The observed data are further complicated by right truncation. At any observation time $D$, we can only observe delays where the secondary event has already occurred. Delays longer than $D - P_u$ are systematically missing from the data, creating a bias toward shorter delays in recent data. This truncation effect is particularly important in real-time analyses during outbreaks, when recent primary events with longer delays are not yet observed.

Combining the above, the generative process for generating $N$ items of delay data with a maximum observable time of $D$ is simulated by following these steps:

1. Choose or generate the delay distribution parameters $\theta$.
2. Choose or generate the primary and secondary window widths: $\{w_P^{(i)}\}_{i = 1,\dots,N}$ and $\{w_S^{(i)}\}_{i = 1,\dots,N}$.
3. For data items $i = 1,\dots, N$:
    a. Generate a copy of the primary event time $P_u^{(i)}$ from its distribution $f_{P_u}(p)$.
    b. Generate a copy of the delay $T^{(i)}$ from the delay distribution $f_{T; \theta}(t)$.
    c. Calculate the secondary event time as $S^{(i)} = P_u^{(i)} + T^{(i)}$.
    d. Apply censoring by observing only that $P_u^{(i)} \in [t^{(i)}_P, t^{(i)}_P + w^{(i)}_P)$ and $S^{(i)} \in [t^{(i)}_S, t^{(i)}_S + w^{(i)}_S)$ where $t^{(i)}_P$ and $t^{(i)}_S$ are:
    
    $$
    \begin{aligned}
    t^{(i)}_P &= \lfloor P_u^{(i)} / w^{(i)}_P \rfloor w^{(i)}_P, \\ 
    t^{(i)}_S &= \lfloor S^{(i)} / w^{(i)}_S \rfloor w^{(i)}_S.
    \end{aligned}
    $$

    e. Apply right truncation by excluding any observations where $S^{(i)} > D$ and returning to step a. until a non excluded observation is generated.

 Note that this generative process is flexible enough to generate data with a mixture of censoring window widths that can vary between primary and secondary events as well as by data item. 

The result of this data generating process is data in the form of $N$ linked primary and secondary censoring intervals, $\left([t^{(i)}_P, t^{(i)}_P + w^{(i)}_P),~ [t^{(i)}_S, t_S^{(i)} + w^{(i)}_S)  \right),~ i = 1,\dots,N$. The simulated censored delays 

$$T_c^{(i)} = t^{(i)}_S  - t^{(i)}_P, \qquad i = 1,\dots, N$$

are correctly sampled from the censored delay PMF given an underlying true delay distribution with parameters $\theta$: $\Pr(T^{(i)}_c = t^{(i)}_S - t^{(i)}_P\mid \theta)$ but without explicitly calculating the censored delay PMF.

#### Log-likelihood of delay data

The generative process above correctly samples from the censored delay PMF, but without calculating the PMF directly. 

The log-likelihood of the data given delay distribution parameters $\theta$ is:

$$
\mathcal{l}(\theta) = \sum_{i=1}^N \ln \Pr(T^{(i)}_c | \theta).
$$ {#eq-loglike}

This means that accurately and efficiently calculating $\Pr(T^{(i)}_c | \theta)$, 
whilst accounting for the various biases induced by censoring and truncation, is both an useful analysis output and required for likelihood-based estimation on $\theta$ given censored and right truncated data. 

The Ward et al. approach approximates solving this by directly recreating the data-generating process in the model using latent variables, adding $2N$ effective parameters to the Bayesian estimation problem [@Park2024.01.12.24301247; @Ward2022-wo]. Below, we solve the censoring problem by converting the problem into single censoring problem with an adjusted delay distribution, which is efficiently solved using marginalisation.


### Solving the censoring problem

#### Double censored and right-truncated PMF

To recover unbiased estimates of the delay distribution parameters from observed data, we begin by considering secondary event censoring with right truncation, and then extend this to include primary event censoring. This is equivalent to having a primary censoring window of zero width, $w_P = 0$.

$$\Pr(T_c = t_S - t_P \mid \text{observed}, P_u = t_P) = \frac{\Pr(S \in [t_S, t_S + w_S] \mid P_u = t_P)}{\Pr(S < D \mid P_u = t_P)}$$

Because the primary time is known, this can be written in terms of the CDF of the true delay distribution $F_T$:

$$
\Pr(T_c = t_S - t_P  \mid \text{observed}, P_u = t_P) = \frac{F_T(t_S + w_S - t_P) - F_T(t_S - t_P)}{F_T(D - t_P)}
$$ {#eq-cdfbasic}

However, in most epidemiological contexts, the primary event is also interval-censored, creating double interval censoring. Following the approach of Park et al. [@Park2024.01.12.24301247], we treat the primary and secondary censoring as separable problems, rather than using the joint interval approach described by Reich et al. [@Reich2009-aa]. This separation significantly simplifies the mathematical formulation.

With primary event censoring, the primary event is known only to have occurred within the interval $[t_P, t_P + w_P]$. Therefore, the observed delay from $t_P$ until the secondary event will be longer than the true delay since it includes the time between the start of the primary window and the primary event time. We denote the duration between the beginning of the primary window, but before the actual primary event time, $A$. The random adjustment $A$ represents the difference between true delay time between the primary and secondary events, $T$, and the adjusted delay time between the beginning of the primary event censor window and the secondary event time, $T_{\text{cens}}$,

$$
\begin{aligned}
A &= P_u - t_P, \\
T_{\text{cens}} &= T + A.
\end{aligned}
$$ {#eq-Tadj}

Although both the true delay $T$ and the adjusted delay $T_{\text{cens}}$ are latent unobserved quantities, recasting the problem in terms of the adjusted delay allows us to separate our analysis into two tractable problems by converting from a double interval censoring problem into a single interval censoring problem with an adjusted delay distribution.

The first problem is relating the observed censoring delay $T_c$ to the unobserved censoring adjusted delay $T_{\text{cens}}$. Because the beginning time of the adjusted delay is fixed to $t_P$ by construction, we can replace the true delay CDF in @eq-cdfbasic above with the adjusted delay distribution,

$$
\Pr(T_c = t_S - t_P \mid \text{observed}) = \frac{F_{\text{cens}}(t_S + w_S - t_P) - F_{\text{cens}}(t_S - t_P)}{F_{\text{cens}}(D - t_P)}.
$$ {#eq-cdfadj}

The second problem is efficiently calculating the cumulative distribution function (CDF) of the adjusted delay $F_{\text{cens}}(q) = \Pr(T_{\text{cens}} < q)$ so that we can apply @eq-cdfadj to constructing the PMF for $T_c$ and in likelihood calculations for estimating $\theta$ (@eq-loglike). In the next section, we focus on this derivation.

#### Primary event censored distributions

Having established the importance of the censoring adjusted delay time CDF ($F_{\text{cens}}$) in the previous section, we now focus on deriving this distribution.

The CDF of the sum of two independent random variables, $X$, $Y$, is 

$$
F_{X+Y}(q) = \mathbb{E}_Y[F_X(q-Y)].
$$ {#eq-sumcdf}  

Applying @eq-sumcdf to @eq-Tadj and using the definition for $P$ @eq-defP,

$$
\begin{aligned}
F_{\text{cens}}(q) &= \mathbb{E}_{P_u \mid P_u \in [t_P, t_P + w_P]}[\Pr(T \leq (q + t_P - P_u)] \\
 &= \mathbb{E}_{P}[\Pr(T \leq (q + t_P - P)]
\end{aligned}
$$

For continuous random variables, expectations are calculated using integrals. This gives us:

$$F_{\text{cens}}(q) = \int_{t_P}^{t_P + w_P} \Pr(T \leq q + t_P - p ) \cdot f_{P}(p) \, dp$$

where $f_P$ is the conditional PDF of the primary event within its censoring window (@eq-condP).

Because $f_P$ is usually expressed in a simple form, c.f. @eq-condP, it is convenient to make the variable substitution $u = p - t_P$ and write $\Pr(T \leq q + t_P - p )$ in terms of the distribution function of delay time $T$

$$F_{\text{cens}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot f_P(t_P + u) \, du.$$ {#eq-Fadj}

This will reduce the @eq-Fadj to a simple form whenever $f_P$ is modelled as a simple distribution.

@eq-Fadj gives us a general solution to the adjusted delay time CDF as a single variable integral problem. Because $F_{\text{cens}}$ is a single variable integral problem, we can use numerical quadrature to efficiently evaluate this integral for any delay distribution with an analytic representation of its distribution function, or derive analytical solutions for specific parametric distributions, as we will explore in the next section.

As shown in @eq-cdfadj, solving for $F_{\text{cens}}$ also solves for the secondary censored delay PMF in an exact manner that properly accounts for the uncertainty in the primary event time. See the SI for an alternative treatment based on the survival functions and connections to the approaches of Park et al. [@Park2024.01.12.24301247], Reich et al. [@Reich2009-aa], and Cori et al. [@Cori2013].

### Analytical solutions

#### Exponentially tilted primary event times

In practice, censoring patterns can be complex, influenced by a variety of factors including circadian rhythms, weekday/weekend reporting differences, and changing epidemic dynamics. We simplify this complexity by modeling the primary event distribution within its censoring window using an exponentially tilted distribution:

$$f_P(t) \propto \exp(r t) \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

where $r$ controls the growth rate (positive values) or decay rate (negative values) of events within the window. This captures the key feature that during epidemic growth, events are more likely to occur near the end of their censoring windows, while during decline, they occur closer to the beginning.

#### Uniform primary event time as a special case

When $r = 0$, the exponentially tilted distribution reduces to the uniform distribution:

$$f_P(t) = \frac{1}{w_P} \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

In this case, our adjusted delay time CDF integral simplifies to:

$$F_{\text{cens}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot \frac{1}{w_P} \, du = \frac{1}{w_P} \int_{0}^{w_P} F_T(q - u) \, du$$

This integral can be evaluated using integration by parts, transforming it into:

$$F_{\text{cens}}(q) = F_T(q) + \frac{1}{w_P} \left[ \int_q^{q+w_P} f_T(z) (z-q) \, dz \right]$$

#### The role of partial expectation in analytical solutions

Here, we encounter what's known as the partial expectation of the delay distribution:

$$\int_q^{q+w_P} z \cdot f_T(z) \, dz$$

This partial expectation calculates the mean contribution from values within the interval [q, q+w_P]. This relationship enables us to develop analytical solutions that avoid numerical integration for distributions with closed-form partial expectations.

#### Distributions with analytical solutions

We have derived analytical solutions for several commonly used distributions:

- **Gamma distribution** with shape $k$ and scale $\theta$: The partial expectation equals $k\theta$ times the CDF of a Gamma(k+1,θ) distribution evaluated over the same interval.

- **Lognormal distribution** with location $\mu$ and scale $\sigma$: The partial expectation equals $e^{\mu+\sigma^2/2}$ times the CDF of a Lognormal(μ+σ²,σ) distribution over the same interval.

- **Weibull distribution** with shape $k$ and scale $\lambda$: The partial expectation relates to incomplete gamma functions through variable substitution.

- **Exponential distribution** as a special case of both Gamma and Weibull distributions with simpler forms.

The complete mathematical derivations are provided in the Supporting Information.

### Software implementation

#### R interface

The `primarycensored` R package implements the analytical and numerical solutions described in previous sections. The package follows R's standard distribution function pattern with density, distribution, quantile, and random generation functions that mirror base R naming conventions. The core functionality is implemented as S3 methods, enabling straightforward extension with new analytical solutions. The interface supports arbitrary delay distributions through their distribution functions (e.g., `lnorm` for lognormal) and includes optimised analytical implementations for the adjusted delay time CDF ($F_{\text{cens}}$) for gamma, lognormal, and Weibull distributions with uniform primary event times. The package also provides utility functions for exponentially tilted primary event distributions. All functions support any mixture of primary and secondary censoring intervals of varying widths, as well as heterogeneous observation times across observations. The package is fully documented and includes examples in the vignettes. All functionality is tested for correctness and performance. It is available on CRAN and GitHub [@primarycensored; @primarycensoredgithub].

#### Stan implementation

To support Bayesian modelling, we developed Stan implementations of the primary censored delay distribution framework [@stan]. These implementations include log probability mass functions for integrating into observation models. The Stan interface maintains compatibility with the R interface while following Stan's syntactic requirements. We provide a complete Stan model for estimating distribution parameters from double-censored data with within-chain parallelisation for improved efficiency [@cmdstanr]. This model supports all distributions available in Stan and serves as a template that users can extend for more complex modelling scenarios. The implementation includes helper functions that facilitate integrating the Stan code into existing workflows through file generation or direct inclusion via Stan's include mechanism. All Stan code is fully tested against the R implementation and documented as a website [@primarycensoredstan]. We provide a vignette demonstrating how to use the Stan tools and another vignette with examples of how to fit Stan models to simulated data [@primarycensoredstantools; @primarycensoredstanexamples].

#### fitdistrplus extensions

We extended the `fitdistrplus` package to handle double-censored data through a wrapper function that enables maximum likelihood estimation of delay distribution parameters without complex dependencies [@fitdistrplus]. This integration again accounts for primary event censoring, secondary event censoring, and right truncation simultaneously. Similar to the core R functionality, the extension supports arbitrary distributions, mixtures of censoring intervals, and heterogeneous observation times. The wrapper accepts data frames with columns specifying bounds for observed delays, censoring window widths, and maximum observable times. We provide a vignette with examples of how to use the wrapper [@primarycensoredfitdistrplus].

### Evaluation

#### Simulated datasets

We generated 36 synthetic datasets with 10000 individuals each, exploring two primary event distributions within censoring windows: uniform distribution (growth rate $r = 0$) and exponentially tilted distribution with growth rate $r = 0.2$. We tested 18 scenarios (2 delay distributions × 3 truncation levels × 3 censoring patterns) for each primary event distribution. For primary event censoring, we generated events with censoring windows ranging from 1 day to 4 days in width. For the delay distributions, we used three distributions with a common mean of 5 days but varying degrees of variance:  

- **Gamma distribution** (shape $k = 5$, scale $\theta = 1$) representing a moderate variance scenario (5 days), for which we have an analytical solution.
- **Lognormal distribution** (location $\mu = 1.5$, scale $\sigma = 0.5$) representing higher variance (10 days), for which we also have an analytical solution.
- **Burr distribution** (shape1 $c = 3$, shape2 $k = 1.5$, scale $\lambda = 4$) representing the highest variance scenario (10 days with a heavier tail), for which we rely on numerical integration.

We then applied three distinct right truncation scenarios to represent different stages of outbreak analysis:

- **No truncation** (retrospective scenario), where all secondary events were observable regardless of delay length, simulating a complete historical dataset.
- **Moderate truncation** (realistic real-time scenario), where secondary events were observable only if they occurred within 10 days of the first primary event.
- **Severe truncation** (challenging real-time scenario), where secondary events were observable only if they occurred within 5 days of the first primary event.

Finally, we applied secondary event censoring with window widths also ranging from 1 day to 4 days, mirroring the primary event censoring approach. This represents the practical reality that both event times are typically known only to the day or reporting period in surveillance systems.

#### Numerical validation

We validated our analytical and numerical solutions by comparing them with Monte Carlo simulations based on the simulated dataset scenarios described above. For each scenario, we generated empirical probability mass functions (PMFs) from simulated data with varying sample sizes (10, 100, 1,000, and 10,000 observations) and then calculated comparable PMFs using the relevant analytical solution, where available as well as the numerical quadrature-based approach. We then visually compared these PMFs. 

To evaluate the computational efficiency of the different methods, we measured the runtime for each method and also the runtime of sampling from the full Monte Carlo approach for 10, 100, 1000 and 10000 observations. We then visualised the relative runtime of each method compared to the full Monte Carlo approach with 10000 observations.

#### Parameter recovery

Using our simulated datasets, we assessed parameter recovery of the primary event censored distribution approach compared to a naive approach that ignores both censoring and truncation, treating observed delays $T_c$ as if they were the true delays $T$.

For each distribution and truncation scenario, we fit models using both Bayesian and maximum likelihood methods. The Bayesian models were implemented in Stan with a primary censored likelihood that accounts for all forms of censoring and truncation, and a naive likelihood that treated observed delays as exact [@stan]. Both used weakly informative priors for shape and scale parameters, with details provided in the Supporting Information. We used the No-U-Turn Sampler with four chains, each having 1000 warm-up and 1000 sampling iterations, and adapt_delta set to 0.95 [@cmdstanr; @betancourt_2017]. For maximum likelihood estimation, we used our fitdistrplus extension to implement both primary censored models and naive models [@fitdistrplus].

We visualised parameter recovery by plotting posterior densities from Bayesian models or point estimates with confidence intervals from maximum likelihood models against the true parameter values for all scenarios. For the Bayesian models, we reported convergence diagnostics using R-hat [@gelman1992inference] and noted any divergent transitions.

### Case study

We used data from the 2014-2016 Sierra Leone Ebola virus disease (EVD) epidemic, previously used in the analysis of Park et al. [@Fang2016; @Park2024.01.12.24301247]. These data contained symptom onset dates and sample test dates for EVD cases in Sierra Leone from May 2014 through September 2015. Following Park et al., we assumed daily censoring intervals for both events, with each day defined from 12:00 AM to 11:59 PM. We estimated the delay distribution from symptom onset to sample test across four 60-day observation windows (0-60, 60-120, 120-180, and 180-240 days after the first symptom onset), conducting both real-time analyses (using only data within each specific window) and retrospective analyses (including all individuals who developed symptoms within the observation period regardless of when they were tested). We compared the performance of our primary censoring adjusted method to the latent variable approach of Ward et al. recommended by Charniga et al. and the naive approach [@charniga2024best; @Ward2022-wo]. We assumed a gamma distribution with priors on the shape $k$ and scale $\theta$ parameters of Gamma(2, 1) and Gamma(2, 1) respectively.

We visualised the estimated shape and scale parameters of the gamma distribution for each of the four observation windows comparing the real-time and retrospective analyses for all methods. We also compared the computational efficiency of the different methods by plotting the effective samples per second for each method and reported any divergences or other issues with convergence for each method [@cmdstanr] stratifying by observation period.

### Implementation details

Our analysis was implemented using the targets package [@targets] for reproducible workflow management. All analyses were performed in R [@R] with data manipulation handled by dplyr [@dplyr] and visualisations created using ggplot2 [@ggplot2] with patchwork [@patchwork] for combining multiple plots. We used cmdstanr [@cmdstanr] as the interface to Stan [@stan] for Bayesian inference. The complete code for all analyses and visualisations, including simulation scripts, parameter recovery evaluations, and case studies, is available at [@primarycensoredpaper-github].

## Results

### Numerical validation

![**Figure 1: Numerical Validation Results.** (A) PMF comparison showing analytical and numerical solutions against Monte Carlo results for gamma, lognormal, and Burr distributions with 10,000 observations. (B) Accuracy metrics showing total variation distance between computed PMFs and Monte Carlo results across all distribution types and truncation scenarios. (C) Computational efficiency comparison showing run time (log scale) across sample sizes, demonstrating substantial performance advantages of our methods over Monte Carlo simulation.](placeholder_figure1.png)

Our analytical and numerical solutions accurately reproduced the probability mass functions (PMFs) generated by Monte Carlo simulations across all censoring and truncation scenarios (Figure 1A). For distributions with analytical solutions (gamma and lognormal), the analytically derived PMFs and numerically integrated PMFs were effectively identical, confirming the correctness of our theoretical derivations. 

The numerical solutions maintained high accuracy even for the Burr distribution, which lacks an analytical solution, validating our quadrature approach for arbitrary distributions (Figure 1B). This consistency persisted across all sample sizes, with expected increases in Monte Carlo variability at smaller sample sizes (see Figure S1 in Supporting Information for detailed comparisons across sample sizes).

Run time comparisons demonstrated substantial efficiency gains with our methods (Figure 1C). The analytical solutions were approximately 100-1000 times faster than full Monte Carlo simulation for datasets with 10,000 observations. Even the numerical integration approach was 10-100 times faster than simulation-based methods. This performance advantage increased with larger datasets, demonstrating the scalability benefits of our approach for large-scale surveillance data. Additional runtime analyses, including scaling with censoring window width, are provided in Figure S2.

### Parameter recovery

![**Figure 2: Parameter Recovery Results.** (A) Gamma distribution parameter recovery showing posterior distributions of shape and scale parameters for primary censored (PC) and naive methods across truncation scenarios. (B) Method comparison across distributions showing relative bias (%) for all methods and distribution types. (C) Effect of censoring window width on parameter bias, showing minimal impact on PC methods compared to increasing bias in naive methods.](placeholder_figure2.png)

Our primary censored distribution approach accurately recovered the true delay distribution parameters across all simulation scenarios (Figure 2A). For gamma distributions, both shape and scale parameters were recovered with minimal bias, even under severe truncation conditions. Similar accuracy was observed for lognormal and Burr distributions, with marginally wider uncertainty intervals for the Burr distribution reflecting its more complex parametrisation (Figure 2B).

In contrast, naive methods that ignored censoring and truncation consistently underestimated the mean of the delay distributions, with bias increasing proportionally to censoring window width and the severity of truncation (Figure 2C). This bias was particularly pronounced in the severe truncation scenario, where the naive approach underestimated the mean delay by up to 40%.

Both Bayesian and maximum likelihood implementations demonstrated similar parameter recovery accuracy. The Bayesian models showed good convergence across all scenarios with R-hat values consistently below 1.01 and no divergent transitions in the MCMC sampling. The Stan implementation also demonstrated computational efficiency, producing more effective samples per second than comparable latent variable approaches. More comprehensive parameter recovery results across all scenarios and detailed MCMC diagnostics are provided in Figures S3-S5.

### Case study

![**Figure 3: Ebola Case Study Results.** (A) Parameter estimates across time periods showing gamma shape vs scale parameter estimates for all methods, with separate panels for real-time and retrospective analyses. (B) Mean delay estimates over time for each method and analysis type, demonstrating consistency of PC and Ward et al. approaches versus bias in naive estimates. (C) Computational performance comparison showing effective samples per second (log scale) across observation periods, highlighting the substantial efficiency advantage of our method.](placeholder_figure3.png)

In the EVD case study, our method estimated symptom onset to sample test delay distributions that closely aligned with those from the latent variable approach of Ward et al. across all observation periods (Figure 3A). Both methods produced similar estimates of the gamma shape and scale parameters, confirming that our marginalisation approach maintains the statistical integrity of the current best practice.

The mean delay estimates from both our primary censored method and the Ward et al. approach remained consistent across real-time and retrospective analyses, while the naive approach showed substantial variation and bias, particularly in real-time analyses of recent data where truncation effects were strongest (Figure 3B).

However, our method demonstrated significant computational advantages, achieving approximately 20-30 times more effective samples per second than the latent variable approach (Figure 3C). This efficiency gain was particularly evident in the later observation periods (120-180 and 180-240 days) where the dataset size was larger.

The differences between real-time and retrospective analyses highlighted the importance of accounting for right truncation in delay distribution estimation. Both our method and the Ward et al. approach showed consistent parameter estimates between real-time and retrospective analyses when properly accounting for truncation, while the naive approach showed significant differences. Complete parameter visualizations across all observation periods, fitted distributions, and detailed computational performance metrics are provided in Figures S6-S7.

## Discussion

### Summary

We have developed a statistically rigorous yet computationally efficient method for estimating delay distributions from double interval censored and right truncated data. Our approach maintains the statistical integrity of current best practices while substantially improving computational performance through analytical marginalisation over primary event times. We demonstrated that our method accurately recovers distribution parameters across various censoring and truncation scenarios, performs substantially faster than existing methods, and provides a standardised framework implemented in accessible software tools.

### Strengths and limitations

Our primary event censored distribution approach offers several key strengths. First, our derivation of analytical solutions for common distributions (gamma, lognormal, Weibull) enables extremely efficient computation without numerical integration in many scenarios. Second, our generalised framework supports any delay distribution with a tractable CDF, extending beyond distributions with analytical solutions through numerical quadrature. Third, our approach explicitly handles right truncation, a critical factor in real-time analyses that is often overlooked or approximated. Finally, our software implementations in both R and Stan provide accessibility to a wide range of users with varying technical expertise.
The main limitation of our approach is that it currently provides analytical solutions only for a relatively simple form of primary event time distributions within censoring windows (uniform), whereas real-world reporting patterns may involve more complex distributions. However, this limitation applies only to analytical solutions, as our framework can handle any arbitrary complexity in primary event time distributions numerically through quadrature methods, maintaining statistical rigour while trading some computational efficiency.

Additionally, while our software supports datasets with censoring intervals that can vary across observations, our evaluation focused exclusively on datasets where all observations share the same window widths. Real-world surveillance data often contains mixtures of reporting intervals (e.g., daily and weekly reporting) that could affect parameter estimation in ways not explored here. Furthermore, we did not fully investigate the interaction between the choice of exponentially tilted versus uniform primary event distributions and the length of primary censoring intervals despite again these options being available in our software. As discussed by Park et al., the importance of using an exponentially tilted verusus a uniform prior depends on both the growth rate of the outbreak and the censoring interval length [@Park2024.01.12.24301247].

It is important to note that our current software implementation in R and Stan, while powerful, is not truly composable due to inherent limitations in these environments. This reduces its potential utility for complex, modular epidemiological models. We aim to address this limitation through a future Julia implementation that would better enable composable modelling approaches.

### Comparison with existing methods

The double interval censoring approach by Reich et al. provides a rigorous statistical framework but lacks analytical solutions and explicit truncation handling [@Reich2009-aa]. Software like coarsedatatools implements this approach but has limited distribution options. Park et al. evaluated several methods and highlighted challenges in real-time estimation [@Park2024.01.12.24301247]. Our work builds on these findings. Jamieson et al. demonstrated the biological plausibility of Burr distributions for incubation periods [@Jamieson2024-kk], but without software implementation or truncation support. Our approach allows the use of Burr distributions, or any other distribution with a CDF function, within a validated statistical framework that supports truncation and primary event censoring. 

The latent variable method developed by Ward et al. and recommended by Charniga et al. maintains statistical integrity but treats primary event times as individual parameters, limiting computational efficiency [@charniga2024best; @Ward2022-wo; @epidist]. Our approach is substantially more computationally efficient and accounts for truncation exactly rather than approximately. Vink et al. developed mixture models for serial intervals [@Vink2014-rq], implemented in the mitey package [@Ainslie-jl] but did not consider truncation. The likelihood we have developed can be used as a component in a mixture model simplifying the implementation of these mixture models. Champredon et al. showed that generation time estimation requires consideration of transmission processes [@Champredon2015-oq], a complexity beyond standard delay distribution methods. Our approach can be used as a likelihood in the joint models required for generation time estimation. 

### Future work

Further development could include analytical solutions for additional distributions and for exponentially tilted primary event distributions. Improved mixture distribution support would enable better access to these models. Future work should also investigate the performance of our methods on datasets with heterogeneous censoring intervals, as real-world surveillance systems often combine different reporting frequencies. Additionally, systematic exploration of the interaction between primary event distribution assumptions (uniform versus exponentially tilted) and censoring interval lengths would provide valuable guidance for practitioners choosing between these approaches based on their specific outbreak context and data characteristics. Work is underway to support regression models for delay parameters through epidist [@epidist], capturing spatial, temporal, or demographic variation. Implementation in Julia could enable composable modelling frameworks that integrate delay distributions with other epidemic components, building on interoperability concepts [@Nicholson2022-ua]. Such developments could make these methods part of a standard outbreak analysis toolbox, helping to address current barriers in settings where models could provide actionable insights for public health decision-making.

### Conclusions

Our primary event censored distribution approach addresses key challenges in epidemiological delay modelling by providing a statistically rigorous yet computationally efficient framework for handling double interval censored data. We maintain the statistical integrity of current best practice approaches whilst improving computational scalability through marginalisation. Our implementations in R and Stan enable straightforward integration with existing epidemiological workflows, and our analytical solutions for common distributions eliminate numerical integration in many scenarios. Results from simulated data and real-world Ebola case data demonstrate that our approach accurately recovers delay distributions across varying censoring scenarios whilst outperforming existing methods in computational efficiency. By establishing this framework and providing accessible software tools, we improve delay distribution estimation in disease surveillance and outbreak analysis, contributing to more accurate and timely public health decision-making.

## References
