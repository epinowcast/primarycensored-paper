---
title: "Modelling delays with primary Event Censored Distributions"
author:
  - name: Samuel P. C. Brand
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: true
  - name: Kelly Charniga
    affiliations:
      - name: Institut Pasteur, France
    corresponding: false
  - name: Sang Woo Park
    affiliations:
      - name: Department of Ecology and Evolution, University of Chicago, Chicago, Illinois, United States of America
    corresponding: false
  - name: James Mba Azam
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Adam Howes
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: false
  - name: Carl Pearson
    affiliations:
      - name: Affiliation
    corresponding: false
  - name: Sebastian Funk
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Sam Abbott
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
    email: sam.abbott@lshtm.ac.uk
date: today
format:
  html:
    toc: false
    number-sections: false
bibliography: reference.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/plos.csl
---

## Abstract

Delay distributions are essential for understanding disease dynamics but are frequently biased by censoring and truncation. Current approaches for handling double interval censored data either discard observations, rely on discretisation, treat unobserved primary events as individual parameters, creating computational barriers for large datasets, or can't readily be extended to account for truncation. We propose a statistically rigorous yet computationally efficient approach that marginalises over latent primary event times rather than estimating them individually. We first develop a general numerical solution and then derive analytical solutions for commonly used distributions including gamma, lognormal and Weibull distributions. We implement these methods in the open-source primarycensored R package with R and Stan interfaces and an extension for the fitdistrplus package. We validate our approach using simulated data. We then apply our method to case data from the 2014-2016 Sierra Leone Ebola epidemic, comparing to the current best practice latent parameter approach. Our method maintains statistical integrity whilst improving scalability for large-scale surveillance datasets, enabling accurate delay distribution estimation where previous approaches become computationally intractable. The standardised framework and accompanying software tools facilitate integration with existing epidemiological models.

## Author summary

## Introduction

Time-to-event distributions are essential in epidemiology, describing delays between events like infection and symptom onset. These inform disease natural history, enable nowcasting, and support epidemic reconstruction. However, real-world surveillance data present challenges due to censoring and truncation, which can significantly bias estimates if not properly addressed [@charniga2024best]. Inaccurate delay distributions directly impact critical response decisions during outbreaks, from resource allocation to intervention timing, potentially leading to mischaracterisation of epidemic dynamics and delayed public health actions [@Park2024.01.12.24301247].

A key challenge in epidemiological data is interval censoring, where event times are known only within specific intervals. Primary event censoring affects the initial event time (e.g., infection), secondary event censoring affects the end event time (e.g., symptom onset), and double interval censoring occurs when both are present [@charniga2024best; @Reich2009-aa]. These issues are compounded by right truncation, where events with longer delays are systematically missing from recent data because the secondary event hasn't yet occurred or been observed [@Park2024.01.12.24301247]. Each form of censoring introduces distinct biases that must be addressed for accurate estimation [@law1992effects].

Several approaches attempt to address these challenges, but each has limitations. Simple methods include discarding censored observations (infeasible when all data are censored) [@little2019statistical], discretisation (treating delays as occurring in fixed intervals), or fixed imputation (assuming events occur at interval midpoints). Park et al. found that treating unobserved primary event times as latent parameters proposed by Ward et al. improved upon fixed imputation [@Park2024.01.12.24301247; @Ward2022-wo], but this introduces N additional parameters when analysing N observations, limiting scalability to large datasets. Charniga et al. highlighted a persistent gap in computationally efficient methods for double interval censored data that maintain statistical rigour while scaling to large surveillance datasets [@charniga2024best].

In this paper, we aim to establish a standard approach for epidemiological delay distribution estimation that is statistically rigourous and computationally efficient. We extend Ward et al.'s method by marginalising over latent primary event times rather than treating them as individual parameters, maintaining statistical integrity while substantially improving scalability for large datasets. We first provide a general numerically solvable solution and then derive analytical solutions for common parametric distributions, including gamma, lognormal, and Weibull. We implement these methods in the open-source R package 'primarycensored' [@abbottprimarycensored], with both R functions and Stan extensions that integrate with existing tools like fitdistrplus and epidemiological modelling packages [@fitdistrplus; @stan; @abbottepinow2; @Cori2013]. In this paper, we present the statistical framework underlying our approach, derive analytical solutions for commonly used distributions, and evaluate our method using both simulated data and real-world case studies across a range of distributions. We first compare numerical accuracy and run time with the Ward et al. approach for recovering PMFs and then demonstrate parameter recovery versus a naive approach. Our method makes accurate delay distribution estimation feasible for large-scale surveillance datasets where previous approaches become computationally intractable. Our software tools enable using these approaches as part of more complex epidemiological models.

## Methods

### Statistical framework

#### Problem statement

For an individual, a primary event (e.g. infection) occurs at time $P_u$, followed by a secondary event (e.g. symptom onset) at time $S$. The true delay between these events is $T = S - P_u$, which follows some parametric density $f_{T; \theta}(t)$ which has distribution parameters $\theta$ and cumulative distribution function $F_{T; \theta}(t)$. The statistical problem is making estimation on the distribution parameters $\theta$ given a collection of $N$ data items each giving linked information about primary and secondary event pairs (e.g. a linelist of cases).   

In practice, however, neither $P_u$ nor $S$ is typically observed. Instead, we observe that the primary event occurred within a window $[t_P, t_P + w_P]$, where $w_P$ is the width of the primary censoring interval (often one day in surveillance data). Similarly, the secondary event is observed to occur within $[t_S, t_S + w_S]$, where $w_S$ is the width of the secondary censoring interval. The censored delay time, $T_c$, is measured from the end of the primary window to the end of the secondary window: $T_c = t_S + w_S - (t_P + w_P)$.

With non-informative censoring (where the observation process is independent of the actual event times), the distribution of the primary event within its window is:

$$f_P(p) = \frac{f_{P_u}(p)}{F_{P_u}(t_P + w_P) - F_{P_u}(t_P)}, \quad p \in [t_P, t_P + w_P]$$

In the simplest case, primary events occur uniformly within their censoring windows, but in real epidemiological situations, they often follow non-uniform patterns. During epidemic growth, for example, events are more likely to occur near the end of their censoring windows, while during decline, they are more likely to occur near the beginning. This can be modeled using an exponentially tilted distribution:

$$f_P(t) \propto \exp(r t) \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

where $r$ controls the growth/decline rate.

The observed data are further complicated by right truncation. At any observation time $D$, we can only observe delays where the secondary event has already occurred. Delays longer than $D - P_u$ are systematically missing from the data, creating a bias toward shorter delays in recent data. This truncation effect is particularly important in real-time analyses during outbreaks, when recent primary events with longer delays are not yet observed.

#### Generative process for delay data

The generative process for generating $N$ items of delay data with a maximum observable time of $D$ we consider in this paper follows these steps:

1. Choose or generate the delay distribution parameters $\theta$.
2. Choose or generate the primary and secondary window widths: $\{w_P^{(i)}\}_{i = 1,\dots,N}$ and $\{w_S^{(i)}\}_{i = 1,\dots,N}$.
3. For data items $i = 1,\dots, N$:
    a. Generate a copy of the primary event time $P_u^{(i)}$ from its distribution $f_{P_u}(p)$.
    b. Generate a copy of the delay $T^{(i)}$ from the delay distribution $f_{T; \theta}(t)$.
    c. Calculate the secondary event time as $S^{(i)} = P_u^{(i)} + T^{(i)}$.
    d. Apply censoring by observing only that $P_u^{(i)} \in [t^{(i)}_P, t^{(i)}_P + w^{(i)}_P)$ and $S^{(i)} \in [t^{(i)}_S, t^{(i)}_S + w^{(i)}_S)$ where $t^{(i)}_P$ and $t^{(i)}_S$ are:
    
    $$
    \begin{aligned}
    t^{(i)}_P &= \lfloor P_u^{(i)} / w^{(i)}_P \rfloor w^{(i)}_P, \\ 
    t^{(i)}_S &= \lfloor S^{(i)} / w^{(i)}_S \rfloor w^{(i)}_S.
    \end{aligned}
    $$

    e. Apply right truncation by excluding any observations where $S^{(i)} > D$ and returning to step a. until a non excluded observation is generated.

Note that this generative process is flexible enough to generate data with a mixture of censoring window widths that can vary between primary and secondary events as well as by data item.

#### Log-likelihood of delay data

The inference problem we are concerned with is when we have data in the form of $N$ linked primary and secondary censoring intervals, $\left([t^{(i)}_P, t^{(i)}_P + w^{(i)}_P),~ [t^{(i)}_S, t_S^{(i)} + w^{(i)}_S)  \right),~ i = 1,\dots,N.$ 

For the generative model described above, the likelihood of this data set only depends on the censored delay times for each data item:

$$T_c^{(i)} = t^{(i)}_S + w^{(i)}_S - (t^{(i)}_P + w^{(i)}_P), \qquad i = 1,\dots, N.$$

The probability mass function (PMF) of the censored delay time $T_c$ is a combination of the true parameteric delay distribution $f_{T;\theta}(t)$ and the censoring and truncation processes. Therefore, the log-likelihood of the data given delay distribution parameters $\theta$ is:

$$
\mathcal{l}(\theta) = \sum_{i=1}^N \ln P(T^{(i)}_c | \theta).
$$


The primary goal of our method is to accurately and efficiently calculate $P(T^{(i)}_c | \theta)$, accounting for the various biases induced by censoring and truncation. We call this the censoring problem.

Solving the censoring problem enables unbiased estimation of the delay distribution parameters $\theta$ either in the frequentist paradigm via numerical optimisation of $\mathcal{l}$, or in the Bayesian paradigm via Markov chain Monte carlo (MCMC) sampling. The Ward et al. approach deals with modelling this sampling process by directly recreating the censored event times as latent variables in the model [@Park2024.01.12.24301247; @Ward2022-wo].


### Solving the censoring problem

#### Double censored and right truncated PMF

To recover the true delay distribution $f_T(t)$ from observed data, we begin by considering the simplest case of secondary event censoring with right truncation, and then extend to include primary event censoring.

When only the secondary event is interval-censored, our goal is to calculate the probability that a delay falls into a specific observed bin. For a secondary event observed in the interval $[t_S, t_S + w_S]$ given that the primary event occurred at a precisely known time $P_u = t_P$, and conditioned on the secondary event occurring before the maximum observable time $D$, the PMF is:

$$P(T_c = t_S + w_S - t_P \mid \text{observed}) = \frac{P(S \in [t_S, t_S + w_S] \mid P_u = t_P)}{P(S < D \mid P_u = t_P)}$$

This can be written in terms of the CDF of the true delay distribution $F_T$:

$$P(T_c = t_S + w_S - t_P \mid \text{observed}) = \frac{F_T(t_S + w_S - t_P) - F_T(t_S - t_P)}{F_T(D - t_P)}$$

However, in most epidemiological contexts, the primary event is also interval-censored, creating double interval censoring. Following the approach of Park et al. [@Park2024.01.12.24301247], we treat the primary and secondary censoring as separable problems, rather than using the joint interval approach described by Reich et al. [@Reich2009-aa]. This separation significantly simplifies the mathematical formulation.

With primary event censoring, the primary event is known only to have occurred within the interval $[t_P, t_P + w_P]$. To account for this additional uncertainty, we need to derive the distribution of delays when the primary event is interval-censored. We introduce $F_{\text{cens}}(q)$ as the cumulative distribution function (CDF) of the delay distribution adjusted for primary event censoring:

$$F_{\text{cens}}(q) = P(T \leq q \mid P \in [t_P, t_P + w_P])$$

Once we have $F_{\text{cens}}$, we can extend our formula for secondary event censoring by substituting $F_{\text{cens}}$ for $F_T$:

$$P(T_c = t_S + w_S - (t_P + w_P) \mid \text{observed}) = \frac{F_{\text{cens}}(t_S + w_S - t_P) - F_{\text{cens}}(t_S - t_P)}{F_{\text{cens}}(D - t_P)}$$

which is equivalent to,

$$P(T_c = t_S + w_S - (t_P + w_P) \mid \text{observed}) = \frac{P(S \in [t_S, t_S + w_S] \mid P \in [t_P, t_P + w_P])}{P(S < D \mid P \in [t_P, t_P + w_P])}$$

This approach breaks the problem of double censoring and right truncation into separable components, with the primary censored CDF ($F_{\text{cens}}$) as the core element that needs to be derived. In the next section, we focus on deriving the primary censored distribution $F_{\text{cens}}$.

#### Primary event censored distributions

Having established the importance of the primary censored CDF ($F_{\text{cens}}$) in the previous section, we now focus on deriving this distribution.

The primary event censored CDF, $F_{\text{cens}}(q)$, represents the probability that the delay is less than or equal to $q$, given that the primary event occurred within its censoring window. By the law of total probability, we can decompose this conditional probability by considering all possible specific times where the primary event might have occurred within the censoring window:

$$F_{\text{cens}}(q) = P(T \leq q \mid P \in [t_P, t_P + w_P])$$

This can be formulated as an expectation over all possible primary event times, essentially calculating a weighted average where each possible primary event time is weighted by its probability:

$$F_{\text{cens}}(q) = \mathbb{E}_{P}[P(T \leq q \mid P)]$$

For continuous random variables, expectations are calculated using integrals. This gives us:

$$F_{\text{cens}}(q) = \int_{t_P}^{t_P + w_P} P(T \leq q \mid P = p) \cdot f_P(p) \, dp$$

where $f_P$ is the PDF of the primary event within its censoring window.

For a primary event occurring at time $p$, the true delay between this event and a secondary event occurring at time $t_P + q$ is $(t_P + q - p)$. Therefore, the conditional probability is:

$$P(T \leq q \mid P = p) = F_T(t_P + q - p)$$

where $F_T$ is the CDF of the true delay distribution. Substituting this into our integral:

$$F_{\text{cens}}(q) = \int_{t_P}^{t_P + w_P} F_T(t_P + q - p) \cdot f_P(p) \, dp$$

For computational simplicity, we can shift the integration variable to measure time from the start of the primary event window. Using the substitution $u = p - t_P$ (time since the start of the window):

$$F_{\text{cens}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot f_P(t_P + u) \, du$$

This formulation gives us a general solution for the primary censored CDF that properly accounts for the uncertainty in the primary event time. We can use numerical integration to evaluate this integral for any delay distribution, or derive analytical solutions for specific parametric distributions, as we will explore in the next section. See the SI for an alternative treatment based on the survival functions and connections to the approaches of Park et al. [@Park2024.01.12.24301247], Reich et al. [@Reich2009-aa], and Cori et al. [@Cori2013].

### Analytical solutions

#### Exponentially tilted primary event times

In practice, censoring patterns can be complex, influenced by a variety of factors including circadian rhythms, weekday/weekend reporting differences, and changing epidemic dynamics. We simplify this complexity by modeling the primary event distribution within its censoring window using an exponentially tilted distribution:

$$f_P(t) \propto \exp(r t) \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

where $r$ controls the growth rate (positive values) or decay rate (negative values) of events within the window. This captures the key feature that during epidemic growth, events are more likely to occur near the end of their censoring windows, while during decline, they occur closer to the beginning.

#### Uniform primary event time as a special case

When $r = 0$, the exponentially tilted distribution reduces to the uniform distribution:

$$f_P(t) = \frac{1}{w_P} \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

In this case, our primary censored CDF integral simplifies to:

$$F_{\text{cens}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot \frac{1}{w_P} \, du = \frac{1}{w_P} \int_{0}^{w_P} F_T(q - u) \, du$$

This integral can be evaluated using integration by parts, transforming it into:

$$F_{\text{cens}}(q) = F_T(q) + \frac{1}{w_P} \left[ \int_q^{q+w_P} f_T(z) (z-q) \, dz \right]$$

#### The role of partial expectation in analytical solutions

Here, we encounter what's known as the partial expectation of the delay distribution:

$$\int_q^{q+w_P} z \cdot f_T(z) \, dz$$

This partial expectation calculates the mean contribution from values within the interval [q, q+w_P]. This relationship enables us to develop analytical solutions that avoid numerical integration for distributions with closed-form partial expectations.

#### Distributions with analytical solutions

We have derived analytical solutions for several commonly used distributions:

- **Gamma distribution** with shape $k$ and scale $\theta$: The partial expectation equals $k\theta$ times the CDF of a Gamma(k+1,θ) distribution evaluated over the same interval.

- **Lognormal distribution** with location $\mu$ and scale $\sigma$: The partial expectation equals $e^{\mu+\sigma^2/2}$ times the CDF of a Lognormal(μ+σ²,σ) distribution over the same interval.

- **Weibull distribution** with shape $k$ and scale $\lambda$: The partial expectation relates to incomplete gamma functions through variable substitution.

- **Exponential distribution** as a special case of both Gamma and Weibull distributions with simpler forms.

The complete mathematical derivations are provided in the Supporting Information.

### Software implementation

#### R interface

The `primarycensored` R package implements the analytical and numerical solutions described in previous sections. The package follows R's standard distribution function pattern with density, distribution, quantile, and random generation functions that mirror base R naming conventions. The core functionality is implemented as S3 methods, enabling straightforward extension with new analytical solutions. The interface supports arbitrary delay distributions through their distribution functions (e.g., `lnorm` for lognormal) and includes optimised analytical implementations for gamma, lognormal, and Weibull distributions with uniform primary event times. The package also provides utility functions for exponentially tilted primary event distributions. All functions support any mixture of primary and secondary censoring intervals of varying widths, as well as heterogeneous observation times across observations. The package is fully documented and includes examples in the vignettes. All functionality is tested for correctness and performance. It is available on CRAN and GitHub [@primarycensored; @primarycensoredgithub].

#### Stan implementation

To support Bayesian modelling, we developed Stan implementations of the primary censored delay distribution framework [@stan]. These implementations include log probability mass functions for integrating into observation models. The Stan interface maintains compatibility with the R interface while following Stan's syntactic requirements. We provide a complete Stan model for estimating distribution parameters from double-censored data with within-chain parallelisation for improved efficiency [@cmdstanr]. This model supports all distributions available in Stan and serves as a template that users can extend for more complex modelling scenarios. The implementation includes helper functions that facilitate integrating the Stan code into existing workflows through file generation or direct inclusion via Stan's include mechanism. All Stan code is fully tested against the R implementation and documented as a website [@primarycensoredstan]. We provide a vignette demonstrating how to use the Stan tools and another vignette with examples of how to fit Stan models to simulated data [@primarycensoredstantools; @primarycensoredstanexamples].

#### fitdistrplus extensions

We extended the `fitdistrplus` package to handle double-censored data through a wrapper function that enables maximum likelihood estimation of delay distribution parameters without complex dependencies [@fitdistrplus]. This integration again accounts for primary event censoring, secondary event censoring, and right truncation simultaneously. Similar to the core R functionality, the extension supports arbitrary distributions, mixtures of censoring intervals, and heterogeneous observation times. The wrapper accepts data frames with columns specifying bounds for observed delays, censoring window widths, and maximum observable times. We provide a vignette with examples of how to use the wrapper [@primarycensoredfitdistrplus].

### Evaluation

#### Simulated datasets

We generated 9 synthetic datasets with 10000 individuals each. For primary event censoring, we generated events with censoring windows ranging from 1 day to 4 days in width. Primary events were assumed to follow a uniform distribution within their censoring windows. For the delay distributions, we used three distributions with a common mean of 5 days but varying degrees of variance:  

- **Gamma distribution** (shape of XX, scale of YY) representing a moderate variance scenario (5 days), for which we have an analytical solution.
- **Lognormal distribution** (location of XX, scale of YY) representing higher variance (10 days), for which we also have an analytical solution.
- **Burr distribution** (shape1 of XX, shape2 of YY, scale of ZZ) representing the highest variance scenario (10 days with a heavier tail), for which we rely on numerical integration.

We then applied three distinct right truncation scenarios to represent different stages of outbreak analysis:

- **No truncation** (retrospective scenario), where all secondary events were observable regardless of delay length, simulating a complete historical dataset.
- **Moderate truncation** (realistic real-time scenario), where secondary events were observable only if they occurred within 10 days of the first primary event.
- **Severe truncation** (challenging real-time scenario), where secondary events were observable only if they occurred within 5 days of the first primary event.

Finally, we applied secondary event censoring with window widths also ranging from 1 day to 4 days, mirroring the primary event censoring approach. This represents the practical reality that both event times are typically known only to the day or reporting period in surveillance systems.

#### Numerical validation

We validated our analytical and numerical solutions by comparing them with Monte Carlo simulations based on the simulated dataset scenarios described above. For each scenario, we generated empirical probability mass functions (PMFs) from simulated data with varying sample sizes (10, 100, 1,000, and 10,000 observations) and then calculated comparable PMFs using the relevant analytical solution where available as well as the numerical quadrature-based approach. We then visually compared these PMFs. 

To evaluate the computational efficiency of the different methods, we measured the runtime for each method and also the runtime of sampling from the full Monte Carlo approach for 10, 100, 1000 and 10000 observations. We then visualised the relative runtime of each method compared to the full Monte Carlo approach with 10000 observations.

#### Parameter recovery

Using our simulated datasets, we assessed parameter recovery of the primary censored distribution approach compared to a naive approach that ignores censoring and truncation.

For each distribution and truncation scenario, we fit models using both Bayesian and maximum likelihood methods. The Bayesian models were implemented in Stan with a primary censored likelihood that accounts for all forms of censoring and truncation, and a naive likelihood that treated observed delays as exact [@stan]. Both used weakly informative priors for shape and scale parameters, with details provided in the Supporting Information. We used the No-U-Turn Sampler with four chains, each having 1000 warm-up and 1000 sampling iterations, and adapt_delta set to 0.95 [@cmdstanr; @betancourt_2017]. For maximum likelihood estimation, we used our fitdistrplus extension to implement both primary censored models and naive models [@fitdistrplus].

We visualised parameter recovery by plotting posterior densities from Bayesian models or point estimates with confidence intervals from maximum likelihood models against the true parameter values for all scenarios. For the Bayesian models, we reported convergence diagnostics using R-hat [@gelman1992inference] and noted any divergent transitions.

### Case study

We used data from the 2014-2016 Sierra Leone Ebola virus disease (EVD) epidemic, previously used in the analysis of Park et al. [[@Fang2016; @Park2024.01.12.24301247]. These data contained symptom onset dates and sample test dates for EVD cases in Sierra Leone from May 2014 through September 2015. Following Park et al., we assumed daily censoring intervals for both events, with each day defined from 12:00 AM to 11:59 PM. We estimated the delay distribution from symptom onset to sample test across four 60-day observation windows (0-60, 60-120, 120-180, and 180-240 days after the first symptom onset), conducting both real-time analyses (using only data within each specific window) and retrospective analyses (including all individuals who developed symptoms within the observation period regardless of when they were tested). We compared the performance of our primary censoring adjusted method to the latent variable approach of Ward et al. recommended by Charniga et al. and the naive approach [@charniga2024best; @Ward2022-wo]. We assumed a gamma distribution with priors on the shape and scale parameters of XX and XX.

We visualised the estimated shape and scale parameters of the gamma distribution for each of the four observation windows comparing the real-time and retrospective analyses for all methods. We also compared the computational efficiency of the different methods by plotting the effective samples per second for each method and reported any divergences or other issues with convergence for each method [@cmdstanr] stratifying by observation period.

### Implementation details

Our analysis was implemented using the targets package [@targets] for reproducible workflow management. All analyses were performed in R [@R] with data manipulation handled by dplyr [@dplyr] and visualisations created using ggplot2 [@ggplot2] with patchwork [@patchwork] for combining multiple plots. We used cmdstanr [@cmdstanr] as the interface to Stan [@stan] for Bayesian inference. The complete code for all analyses and visualisations, including simulation scripts, parameter recovery evaluations, and case studies, is available at [@primarycensoredpaper-github].

## Results

### Numerical validation results
- Accuracy comparison across scenarios and sample sizes
- Runtime performance comparison

### Parameter recovery results
- Stan-based estimation results across all scenarios and sample sizes
- fitdistrplus-based estimation results
- Comparison with naive models in both
- Need to have a main figure with a single sample size and then duplicates for different sample sizes in the SI for both.

### Case study results
- Estimated parameters and key findings
- Comparison with previous estimates from Park et al.
- Performance advantages demonstrated

## Discussion

### Summary

### Strengths and limitations


### Comparison with existing methods

The double interval censoring approach by Reich et al. provides a rigorous statistical framework but lacks analytical solutions and explicit truncation handling [@Reich2009-aa]. Software like coarsedatatools implements this approach but has limited distribution options. Park et al. evaluated several methods and highlighted challenges in real-time estimation [@Park2024.01.12.24301247]. Our work builds on these findings. Jamieson et al. demonstrated the biological plausibility of Burr distributions for incubation periods [@Jamieson2024-kk], but without software implementation or truncation support. Our approach allows the use of Burr distributions, or any other distribution with a CDF function, within a validated statistical framework that supports truncation and primary event censoring. The latent variable method developed by Ward et al. and recommended by Charniga et al. maintains statistical integrity but treats primary event times as individual parameters, limiting computational efficiency [@charniga2024best; @Ward2022-wo; @epidist]. Our approach is substantially more computationally efficient and accounts for truncation exactly rather than approximately. Vink et al. developed mixture models for serial intervals [@Vink2014-rq], implemented in the mitey package [@Ainslie-jl] but did not consider truncation. The likelihood we have developed can be used as a component in a mixture model simplifying the implementation of these mixture models. Champredon et al. showed that generation time estimation requires consideration of transmission processes [@Champredon2015-oq], a complexity beyond standard delay distribution methods. Our approach can be used as a likelihood in the joint models required for generation time estimation. 

### Future work

Further development could include analytical solutions for additional distributions and for exponentially tilted primary event distributions. Improved mixture distribution support would enable better access to these models. Work is underway to support regression models for delay parameters through epidist [@epidist], capturing spatial, temporal, or demographic variation. Implementation in Julia could enable composable modelling frameworks that integrate delay distributions with other epidemic components, building on interoperability concepts [@Nicholson2022-ua]. Such developments could make these methods part of a standard outbreak analysis toolbox, helping to address current barriers in settings where models could provide actionable insights for public health decision-making.

### Conclusions

Our primary event censored distribution approach addresses key challenges in epidemiological delay modelling by providing a statistically rigorous yet computationally efficient framework for handling double interval censored data. We maintain the statistical integrity of current best practice approaches whilst improving computational scalability through marginalisation. Our implementations in R and Stan enable straightforward integration with existing epidemiological workflows, and our analytical solutions for common distributions eliminate numerical integration in many scenarios. Results from simulated data and real-world Ebola case data demonstrate that our approach accurately recovers delay distributions across varying censoring scenarios whilst outperforming existing methods in computational efficiency. By establishing this framework and providing accessible software tools, we improve delay distribution estimation in disease surveillance and outbreak analysis, contributing to more accurate and timely public health decision-making.

## References

## Supporting information

### Detailed analytical solutions

#### General framework for analytical solutions
- Explanation of when analytical solutions are possible
- Role of partial expectation in deriving solutions

#### Solutions for exponentially tilted primary event times
- Complete derivation of equation for F_CP
- Limiting case for uniform distribution

#### Gamma distribution
- Full derivation of partial expectation
- Complete survival function and PMF formulations

#### Lognormal distribution
- Full derivation of partial expectation
- Complete survival function and PMF formulations

#### Weibull distribution
- Full derivation of partial expectation 
- Complete survival function and PMF formulations

### Mathematical details of the naive comparison model
- Derivation of the model used for comparison
- Explanation of the limitations and assumptions
- Implementation details


### Connection to other approaches
- Connection to Park et al.
- Connection to Reich et al. 
- Connection to Cori et al.

### Extended numerical validation results
- Detailed accuracy metrics across all distributions and sample sizes
- Additional visualizations

### Extended parameter recovery results

#### Stan-based estimation
- Full results across all sample sizes
- Detailed convergence diagnostics
- Additional parameter visualizations

#### fitdistrplus-based estimation
- Full results across all sample sizes
- Convergence and optimization details

#### Comparison with naive models
- Complete comparative analysis
- Additional error metrics

### Extended case study results
- Complete parameter visualizations
- Additional comparative outputs
