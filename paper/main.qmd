---
title: "Modelling delays with primary Event Censored Distributions"
author:
  - name: Samuel P. C. Brand
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: true
  - name: Kelly Charniga
    affiliations:
      - name: Institut Pasteur, France
    corresponding: false
  - name: Sang Woo Park
    affiliations:
      - name: Department of Ecology and Evolution, University of Chicago, Chicago, Illinois, United States of America
    corresponding: false
  - name: James Mba Azam
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Adam Howes
    affiliations:
      - name: Center for Forecasting and Outbreak Analytics, Centers for Disease Control and Prevention, United States of America
    corresponding: false
  - name: Carl Pearson
    affiliations:
      - name: Affiliation
    corresponding: false
  - name: Sebastian Funk
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
  - name: Sam Abbott
    affiliations:
      - name: Centre for Mathematical Modelling of Infectious Diseases, London School of Hygiene & Tropical Medicine, London, United Kingdom
    corresponding: false
    email: sam.abbott@lshtm.ac.uk
date: today
format:
  html:
    toc: false
    number-sections: false
bibliography: reference.bib
csl: https://raw.githubusercontent.com/citation-style-language/styles/master/plos.csl
---

## Abstract

Delay distributions are essential for understanding disease dynamics but are frequently biased by censoring and truncation. Current approaches for handling double interval censored data either discard observations, rely on discretisation, treat unobserved primary events as individual parameters, creating computational barriers for large datasets, or can't readily be extended to account for truncation. We propose a statistically rigorous yet computationally efficient approach that marginalises over latent primary event times rather than estimating them individually. We first develop a general numerical solution and then derive analytical solutions for commonly used distributions including gamma, lognormal and Weibull distributions. We implement these methods in the open-source primarycensored R package with R and Stan interfaces and an extension for the fitdistrplus package. We validate our approach using simulated data. We then apply our method to case data from the 2014-2016 Sierra Leone Ebola epidemic, comparing to the current best practice latent parameter approach. Our method maintains statistical integrity whilst improving scalability for large-scale surveillance datasets, enabling accurate delay distribution estimation where previous approaches become computationally intractable. The standardised framework and accompanying software tools facilitate integration with existing epidemiological models.

## Author summary

## Introduction

Time-to-event distributions are essential in epidemiology, describing delays between events like infection and symptom onset. These inform disease natural history, enable nowcasting, and support epidemic reconstruction. However, real-world surveillance data present challenges due to censoring and truncation, which can significantly bias estimates if not properly addressed [@charniga2024best]. Inaccurate delay distributions directly impact critical response decisions during outbreaks, from resource allocation to intervention timing, potentially leading to mischaracterisation of epidemic dynamics and delayed public health actions [@Park2024.01.12.24301247].

A key challenge in epidemiological data is interval censoring, where event times are known only within specific intervals. Primary event censoring affects the initial event time (e.g., infection), secondary event censoring affects the end event time (e.g., symptom onset), and double interval censoring occurs when both are present [@charniga2024best; @Reich2009-aa]. These issues are compounded by right truncation, where events with longer delays are systematically missing from recent data because the secondary event hasn't yet occurred or been observed [@Park2024.01.12.24301247]. Each form of censoring introduces distinct biases that must be addressed for accurate estimation [@law1992effects].

Several approaches attempt to address these challenges, but each has limitations. Simple methods include discarding censored observations (infeasible when all data are censored) [@little2019statistical], discretisation (treating delays as occurring in fixed intervals), or fixed imputation (assuming events occur at interval midpoints). Park et al. found that treating unobserved primary event times as latent parameters proposed by Ward et al. improved upon fixed imputation [@Park2024.01.12.24301247; @Ward2022-wo], but this introduces N additional parameters when analysing N observations, limiting scalability to large datasets. Charniga et al. highlighted a persistent gap in computationally efficient methods for double interval censored data that maintain statistical rigour while scaling to large surveillance datasets [@charniga2024best].

In this paper, we aim to establish a standard approach for epidemiological delay distribution estimation that is statistically rigourous and computationally efficient. We extend Ward et al.'s method by marginalising over latent primary event times rather than treating them as individual parameters, maintaining statistical integrity while substantially improving scalability for large datasets. We first show that when the primary event time is known, the observed delay PMF can be calculated as the increase in cumulative distribution function (CDF) of the true delay distribution over the secondary event censoring window. We then show that marginalising over the primary event time maintains this relationship, albeit with an adjusted CDF that accounts for the primary event censoring. We provide a general numerically solvable solution for the adjusted CDF and then derive analytical solutions for common parametric distributions, including gamma, lognormal, and Weibull. We implement these methods in the open-source R package 'primarycensored' [@abbottprimarycensored], with both R functions and Stan extensions that integrate with existing tools like fitdistrplus and epidemiological modelling packages [@fitdistrplus; @stan; @abbottepinow2; @Cori2013]. In this paper, we present the statistical framework underlying our approach, derive analytical solutions for commonly used distributions, and evaluate our method using both simulated data and real-world case studies across a range of distributions. We first compare numerical accuracy and run time with the Ward et al. approach for recovering PMFs and then demonstrate parameter recovery versus a naive approach. Our method makes accurate delay distribution estimation feasible for large-scale surveillance datasets where previous approaches become computationally intractable. Our software tools enable using these approaches as part of more complex epidemiological models.

## Methods

### Statistical framework

#### Problem statement

For an individual, a primary event (e.g. infection) occurs at time $P_u$, followed by a secondary event (e.g. symptom onset) at time $S$. The true delay between these events is $T = S - P_u$, which follows some density $f_{T; \theta}(t)$ which has distribution parameters $\theta$ and a cumulative distribution function $F_{T; \theta}(t)$. The statistical problem is estimating the distribution parameters $\theta$ given a collection of $N$ data items, each giving linked information about primary and secondary event pairs (e.g. a linelist of cases).   

In practice, however, neither $P_u$ nor $S$ is typically observed. Instead, we observe that the primary event occurred within a window $[t_P, t_P + w_P]$, where $w_P$ is the width of the primary censoring interval (often one day in surveillance data). Similarly, the secondary event is observed to occur within $[t_S, t_S + w_S]$, where $w_S$ is the width of the secondary censoring interval. The censored delay time, $T_c$, is measured from the start of the primary window to the start of the secondary window: $T_c = t_S - t_P$.

The statistical problem is two-fold: first, to correctly impute a posterior distribution for the parameters of the underlying true delay distribution $\theta$ in a way that accounts for censoring and right truncation. Second, to use the posterior distribution for $\theta$ to calculate the probability mass function (PMF) for the possible values of $T_c$.

#### Generative process for delay data

With non-informative censoring (where the observation process is independent of the actual event times), the distribution of the primary event within its window is:

$$f_P(p) = \frac{f_{P_u}(p)}{F_{P_u}(t_P + w_P) - F_{P_u}(t_P)}, \quad p \in [t_P, t_P + w_P]$$

In the simplest case, primary events occur uniformly within their censoring windows, but in real epidemiological situations, they often follow non-uniform patterns. During epidemic growth, for example, events are more likely to occur near the end of their censoring windows, while during decline, they are more likely to occur near the beginning. This can be modeled using an exponentially tilted distribution:

$$f_P(t) \propto \exp(r t) \mathbb{1}_{[t_P, t_P + w_P]}(t)$$ {#eq-condP}

where $r$ controls the growth/decline rate.

The observed data are further complicated by right truncation. At any observation time $D$, we can only observe delays where the secondary event has already occurred. Delays longer than $D - P_u$ are systematically missing from the data, creating a bias toward shorter delays in recent data. This truncation effect is particularly important in real-time analyses during outbreaks, when recent primary events with longer delays are not yet observed.

Combining the above, the generative process for generating $N$ items of delay data with a maximum observable time of $D$ we consider in this paper follows these steps:

1. Choose or generate the delay distribution parameters $\theta$.
2. Choose or generate the primary and secondary window widths: $\{w_P^{(i)}\}_{i = 1,\dots,N}$ and $\{w_S^{(i)}\}_{i = 1,\dots,N}$.
3. For data items $i = 1,\dots, N$:
    a. Generate a copy of the primary event time $P_u^{(i)}$ from its distribution $f_{P_u}(p)$.
    b. Generate a copy of the delay $T^{(i)}$ from the delay distribution $f_{T; \theta}(t)$.
    c. Calculate the secondary event time as $S^{(i)} = P_u^{(i)} + T^{(i)}$.
    d. Apply censoring by observing only that $P_u^{(i)} \in [t^{(i)}_P, t^{(i)}_P + w^{(i)}_P)$ and $S^{(i)} \in [t^{(i)}_S, t^{(i)}_S + w^{(i)}_S)$ where $t^{(i)}_P$ and $t^{(i)}_S$ are:
    
    $$
    \begin{aligned}
    t^{(i)}_P &= \lfloor P_u^{(i)} / w^{(i)}_P \rfloor w^{(i)}_P, \\ 
    t^{(i)}_S &= \lfloor S^{(i)} / w^{(i)}_S \rfloor w^{(i)}_S.
    \end{aligned}
    $$

    e. Apply right truncation by excluding any observations where $S^{(i)} > D$ and returning to step a. until a non excluded observation is generated.

Note that this generative process is flexible enough to generate data with a mixture of censoring window widths that can vary between primary and secondary events as well as by data item. 

The result of this data generating process is data in the form of $N$ linked primary and secondary censoring intervals, $\left([t^{(i)}_P, t^{(i)}_P + w^{(i)}_P),~ [t^{(i)}_S, t_S^{(i)} + w^{(i)}_S)  \right),~ i = 1,\dots,N.$.

#### Log-likelihood of delay data

The likelihood of these data only depends on the censored delay times for each data item:

$$T_c^{(i)} = t^{(i)}_S  - t^{(i)}_P, \qquad i = 1,\dots, N.$$

The probability mass function (PMF) of the censored delay time $T_c$ is a combination of the true parameteric delay distribution $f_{T;\theta}(t)$ and the censoring and truncation processes. Therefore, the log-likelihood of the data given delay distribution parameters $\theta$ is:

$$
\mathcal{l}(\theta) = \sum_{i=1}^N \ln P(T^{(i)}_c | \theta).
$$ {#eq-loglike}

This means the primary goal is to accurately and efficiently calculate $P(T^{(i)}_c | \theta)$, accounting for the various biases induced by censoring and truncation.

The Ward et al. approach approximates solving this by directly recreating the data-generating process in the model using latent variables, adding $2N$ effective parameters to the Bayesian estimation problem [@Park2024.01.12.24301247; @Ward2022-wo].


### Solving the censoring problem

#### Double censored and right truncated PMF

To recover unbiased estimates of the delay distribution parameters from observed data, we begin by considering secondary event censoring with right truncation, and then extend this to include primary event censoring. This is equivalent to having a primary censoring window of zero width, $w_P = 0$.

$$P(T_c = t_S - t_P \mid \text{observed}, P_u = t_P) = \frac{P(S \in [t_S, t_S + w_S] \mid P_u = t_P)}{P(S < D \mid P_u = t_P)}$$

Because the primary time is known, this can be written in terms of the CDF of the true delay distribution $F_T$:

$$
P(T_c = t_S - t_P  \mid \text{observed}, P_u = t_P) = \frac{F_T(t_S + w_S - t_P) - F_T(t_S - t_P)}{F_T(D - t_P)}
$$ {#eq-cdfbasic}

However, in most epidemiological contexts, the primary event is also interval-censored, creating double interval censoring. Following the approach of Park et al. [@Park2024.01.12.24301247], we treat the primary and secondary censoring as separable problems, rather than using the joint interval approach described by Reich et al. [@Reich2009-aa]. This separation significantly simplifies the mathematical formulation.

With primary event censoring, the primary event is known only to have occurred within the interval $[t_P, t_P + w_P]$. Therefore, the observed delay from $t_P$ until the secondary event will be longer than the true delay since it includes the time between the start of the primary window and the primary event time. We denote the duration between the beginning of the primary window, but before the actual primary event time, $A$. The random adjustment $A$ represents the difference between true delay time between the primary and secondary events, $T$, and the adjusted delay time between the beginning of the primary event censor window and the true delay time, $T_{\text{adj}}$,

$$
\begin{aligned}
A &= P_u - t_P, \\
T_{\text{adj}} &= T + A.
\end{aligned}
$$ {#eq-Tadj}

Although both the true delay $T$ and the adjusted delay $T_{\text{adj}}$ are latent unobserved quantities, recasting the problem in terms of the adjusted delay allows us to separate our analysis into two tractable problems.

The first problem is relating the observed censoring delay $T_c$ to the unobserved adjusted delay $T_{\text{adj}}$. Because the beginning time of the adjusted delay is fixed to $t_P$ by construction, we can replace the true delay CDF in @eq-cdfbasic above with the adjusted delay distribution,

$$
P(T_c = t_S - t_P \mid \text{observed}) = \frac{F_{\text{adj}}(t_S + w_S - t_P) - F_{\text{adj}}(t_S - t_P)}{F_{\text{adj}}(D - t_P)}.
$$ {#eq-cdfadj}

The second problem is efficiently calculating the cumulative distribution function (CDF) of the adjusted delay $F_{\text{adj}}(q) = P(T_{\text{adj}} < q)$ so that we can apply @eq-cdfadj to constructing the PMF for $T_c$ and in likelihood calculations for estimating $\theta$ (@eq-loglike). In the next section, we focus on this derivation.

#### Primary event censored distributions

Having established the importance of the adjusted delay time CDF ($F_{\text{adj}}$) in the previous section, we now focus on deriving this distribution.

The CDF of the sum of two independent random variables, $X$, $Y$, is 

$$
F_{X+Y}(q) = \mathbb{E}_Y[F_X(q-Y)].
$$ {#eq-sumcdf}  

Applying @eq-sumcdf to @eq-Tadj gives

$$F_{\text{adj}}(q) = \mathbb{E}_{P_u \mid P_u \in [t_P, t_P + w_P]}[P(T \leq (q + t_P - P_u)]$$

For continuous random variables, expectations are calculated using integrals. This gives us:

$$F_{\text{adj}}(q) = \int_{t_P}^{t_P + w_P} P(T \leq q + t_P - p ) \cdot f_{P}(p) \, dp$$

where $f_P$ is the conditional PDF of the primary event within its censoring window (@eq-condP).

It is convenient to make the variable substitution $u = p - t_P$ and write $P(T \leq q + t_P - p )$ in terms of the distribution function of delay time $T$

$$F_{\text{adj}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot f_P(t_P + u) \, du.$$ {#eq-Fadj}

@eq-Fadj gives us a general solution to the adjusted delay time CDF as a single variable integral problem. Because $F_{\text{adj}}$ is a single variable integral problem, we can use numerical quadrature to efficiently evaluate this integral for any delay distribution with an analytic representation of its distribution function, or derive analytical solutions for specific parametric distributions, as we will explore in the next section.

As shown in @eq-cdfadj, solving for $F_{\text{adj}}$ also solves for the censored delay PMF in an exact manner that properly accounts for the uncertainty in the primary event time. See the SI for an alternative treatment based on the survival functions and connections to the approaches of Park et al. [@Park2024.01.12.24301247], Reich et al. [@Reich2009-aa], and Cori et al. [@Cori2013].

### Analytical solutions

#### Exponentially tilted primary event times

In practice, censoring patterns can be complex, influenced by a variety of factors including circadian rhythms, weekday/weekend reporting differences, and changing epidemic dynamics. We simplify this complexity by modeling the primary event distribution within its censoring window using an exponentially tilted distribution:

$$f_P(t) \propto \exp(r t) \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

where $r$ controls the growth rate (positive values) or decay rate (negative values) of events within the window. This captures the key feature that during epidemic growth, events are more likely to occur near the end of their censoring windows, while during decline, they occur closer to the beginning.

#### Uniform primary event time as a special case

When $r = 0$, the exponentially tilted distribution reduces to the uniform distribution:

$$f_P(t) = \frac{1}{w_P} \mathbb{1}_{[t_P, t_P + w_P]}(t)$$

In this case, our adjusted delay time CDF integral simplifies to:

$$F_{\text{adj}}(q) = \int_{0}^{w_P} F_T(q - u) \cdot \frac{1}{w_P} \, du = \frac{1}{w_P} \int_{0}^{w_P} F_T(q - u) \, du$$

This integral can be evaluated using integration by parts, transforming it into:

$$F_{\text{cens}}(q) = F_T(q) + \frac{1}{w_P} \left[ \int_q^{q+w_P} f_T(z) (z-q) \, dz \right]$$

#### The role of partial expectation in analytical solutions

Here, we encounter what's known as the partial expectation of the delay distribution:

$$\int_q^{q+w_P} z \cdot f_T(z) \, dz$$

This partial expectation calculates the mean contribution from values within the interval [q, q+w_P]. This relationship enables us to develop analytical solutions that avoid numerical integration for distributions with closed-form partial expectations.

#### Distributions with analytical solutions

We have derived analytical solutions for several commonly used distributions:

- **Gamma distribution** with shape $k$ and scale $\theta$: The partial expectation equals $k\theta$ times the CDF of a Gamma(k+1,θ) distribution evaluated over the same interval.

- **Lognormal distribution** with location $\mu$ and scale $\sigma$: The partial expectation equals $e^{\mu+\sigma^2/2}$ times the CDF of a Lognormal(μ+σ²,σ) distribution over the same interval.

- **Weibull distribution** with shape $k$ and scale $\lambda$: The partial expectation relates to incomplete gamma functions through variable substitution.

- **Exponential distribution** as a special case of both Gamma and Weibull distributions with simpler forms.

The complete mathematical derivations are provided in the Supporting Information.

### Software implementation

#### R interface

The `primarycensored` R package implements the analytical and numerical solutions described in previous sections. The package follows R's standard distribution function pattern with density, distribution, quantile, and random generation functions that mirror base R naming conventions. The core functionality is implemented as S3 methods, enabling straightforward extension with new analytical solutions. The interface supports arbitrary delay distributions through their distribution functions (e.g., `lnorm` for lognormal) and includes optimised analytical implementations for gamma, lognormal, and Weibull distributions with uniform primary event times. The package also provides utility functions for exponentially tilted primary event distributions. All functions support any mixture of primary and secondary censoring intervals of varying widths, as well as heterogeneous observation times across observations. The package is fully documented and includes examples in the vignettes. All functionality is tested for correctness and performance. It is available on CRAN and GitHub [@primarycensored; @primarycensoredgithub].

#### Stan implementation

To support Bayesian modelling, we developed Stan implementations of the primary censored delay distribution framework [@stan]. These implementations include log probability mass functions for integrating into observation models. The Stan interface maintains compatibility with the R interface while following Stan's syntactic requirements. We provide a complete Stan model for estimating distribution parameters from double-censored data with within-chain parallelisation for improved efficiency [@cmdstanr]. This model supports all distributions available in Stan and serves as a template that users can extend for more complex modelling scenarios. The implementation includes helper functions that facilitate integrating the Stan code into existing workflows through file generation or direct inclusion via Stan's include mechanism. All Stan code is fully tested against the R implementation and documented as a website [@primarycensoredstan]. We provide a vignette demonstrating how to use the Stan tools and another vignette with examples of how to fit Stan models to simulated data [@primarycensoredstantools; @primarycensoredstanexamples].

#### fitdistrplus extensions

We extended the `fitdistrplus` package to handle double-censored data through a wrapper function that enables maximum likelihood estimation of delay distribution parameters without complex dependencies [@fitdistrplus]. This integration again accounts for primary event censoring, secondary event censoring, and right truncation simultaneously. Similar to the core R functionality, the extension supports arbitrary distributions, mixtures of censoring intervals, and heterogeneous observation times. The wrapper accepts data frames with columns specifying bounds for observed delays, censoring window widths, and maximum observable times. We provide a vignette with examples of how to use the wrapper [@primarycensoredfitdistrplus].

### Evaluation

#### Simulated datasets

We generated 9 synthetic datasets with 10000 individuals each. For primary event censoring, we generated events with censoring windows ranging from 1 day to 4 days in width. Primary events were assumed to follow a uniform distribution within their censoring windows. For the delay distributions, we used three distributions with a common mean of 5 days but varying degrees of variance:  

- **Gamma distribution** (shape of XX, scale of YY) representing a moderate variance scenario (5 days), for which we have an analytical solution.
- **Lognormal distribution** (location of XX, scale of YY) representing higher variance (10 days), for which we also have an analytical solution.
- **Burr distribution** (shape1 of XX, shape2 of YY, scale of ZZ) representing the highest variance scenario (10 days with a heavier tail), for which we rely on numerical integration.

We then applied three distinct right truncation scenarios to represent different stages of outbreak analysis:

- **No truncation** (retrospective scenario), where all secondary events were observable regardless of delay length, simulating a complete historical dataset.
- **Moderate truncation** (realistic real-time scenario), where secondary events were observable only if they occurred within 10 days of the first primary event.
- **Severe truncation** (challenging real-time scenario), where secondary events were observable only if they occurred within 5 days of the first primary event.

Finally, we applied secondary event censoring with window widths also ranging from 1 day to 4 days, mirroring the primary event censoring approach. This represents the practical reality that both event times are typically known only to the day or reporting period in surveillance systems.

#### Numerical validation

We validated our analytical and numerical solutions by comparing them with Monte Carlo simulations based on the simulated dataset scenarios described above. For each scenario, we generated empirical probability mass functions (PMFs) from simulated data with varying sample sizes (10, 100, 1,000, and 10,000 observations) and then calculated comparable PMFs using the relevant analytical solution where available as well as the numerical quadrature-based approach. We then visually compared these PMFs. 

To evaluate the computational efficiency of the different methods, we measured the runtime for each method and also the runtime of sampling from the full Monte Carlo approach for 10, 100, 1000 and 10000 observations. We then visualised the relative runtime of each method compared to the full Monte Carlo approach with 10000 observations.

#### Parameter recovery

Using our simulated datasets, we assessed parameter recovery of the primary censored distribution approach compared to a naive approach that ignores censoring and truncation.

For each distribution and truncation scenario, we fit models using both Bayesian and maximum likelihood methods. The Bayesian models were implemented in Stan with a primary censored likelihood that accounts for all forms of censoring and truncation, and a naive likelihood that treated observed delays as exact [@stan]. Both used weakly informative priors for shape and scale parameters, with details provided in the Supporting Information. We used the No-U-Turn Sampler with four chains, each having 1000 warm-up and 1000 sampling iterations, and adapt_delta set to 0.95 [@cmdstanr; @betancourt_2017]. For maximum likelihood estimation, we used our fitdistrplus extension to implement both primary censored models and naive models [@fitdistrplus].

We visualised parameter recovery by plotting posterior densities from Bayesian models or point estimates with confidence intervals from maximum likelihood models against the true parameter values for all scenarios. For the Bayesian models, we reported convergence diagnostics using R-hat [@gelman1992inference] and noted any divergent transitions.

### Case study

We used data from the 2014-2016 Sierra Leone Ebola virus disease (EVD) epidemic, previously used in the analysis of Park et al. [[@Fang2016; @Park2024.01.12.24301247]. These data contained symptom onset dates and sample test dates for EVD cases in Sierra Leone from May 2014 through September 2015. Following Park et al., we assumed daily censoring intervals for both events, with each day defined from 12:00 AM to 11:59 PM. We estimated the delay distribution from symptom onset to sample test across four 60-day observation windows (0-60, 60-120, 120-180, and 180-240 days after the first symptom onset), conducting both real-time analyses (using only data within each specific window) and retrospective analyses (including all individuals who developed symptoms within the observation period regardless of when they were tested). We compared the performance of our primary censoring adjusted method to the latent variable approach of Ward et al. recommended by Charniga et al. and the naive approach [@charniga2024best; @Ward2022-wo]. We assumed a gamma distribution with priors on the shape and scale parameters of XX and XX.

We visualised the estimated shape and scale parameters of the gamma distribution for each of the four observation windows comparing the real-time and retrospective analyses for all methods. We also compared the computational efficiency of the different methods by plotting the effective samples per second for each method and reported any divergences or other issues with convergence for each method [@cmdstanr] stratifying by observation period.

### Implementation details

Our analysis was implemented using the targets package [@targets] for reproducible workflow management. All analyses were performed in R [@R] with data manipulation handled by dplyr [@dplyr] and visualisations created using ggplot2 [@ggplot2] with patchwork [@patchwork] for combining multiple plots. We used cmdstanr [@cmdstanr] as the interface to Stan [@stan] for Bayesian inference. The complete code for all analyses and visualisations, including simulation scripts, parameter recovery evaluations, and case studies, is available at [@primarycensoredpaper-github].

## Results

### Numerical validation results
- Accuracy comparison across scenarios and sample sizes
- Runtime performance comparison

### Parameter recovery results
- Stan-based estimation results across all scenarios and sample sizes
- fitdistrplus-based estimation results
- Comparison with naive models in both
- Need to have a main figure with a single sample size and then duplicates for different sample sizes in the SI for both.

### Case study results
- Estimated parameters and key findings
- Comparison with previous estimates from Park et al.
- Performance advantages demonstrated

## Discussion

### Summary

### Strengths and limitations


### Comparison with existing methods

The double interval censoring approach by Reich et al. provides a rigorous statistical framework but lacks analytical solutions and explicit truncation handling [@Reich2009-aa]. Software like coarsedatatools implements this approach but has limited distribution options. Park et al. evaluated several methods and highlighted challenges in real-time estimation [@Park2024.01.12.24301247]. Our work builds on these findings. Jamieson et al. demonstrated the biological plausibility of Burr distributions for incubation periods [@Jamieson2024-kk], but without software implementation or truncation support. Our approach allows the use of Burr distributions, or any other distribution with a CDF function, within a validated statistical framework that supports truncation and primary event censoring. The latent variable method developed by Ward et al. and recommended by Charniga et al. maintains statistical integrity but treats primary event times as individual parameters, limiting computational efficiency [@charniga2024best; @Ward2022-wo; @epidist]. Our approach is substantially more computationally efficient and accounts for truncation exactly rather than approximately. Vink et al. developed mixture models for serial intervals [@Vink2014-rq], implemented in the mitey package [@Ainslie-jl] but did not consider truncation. The likelihood we have developed can be used as a component in a mixture model simplifying the implementation of these mixture models. Champredon et al. showed that generation time estimation requires consideration of transmission processes [@Champredon2015-oq], a complexity beyond standard delay distribution methods. Our approach can be used as a likelihood in the joint models required for generation time estimation. 

### Future work

Further development could include analytical solutions for additional distributions and for exponentially tilted primary event distributions. Improved mixture distribution support would enable better access to these models. Work is underway to support regression models for delay parameters through epidist [@epidist], capturing spatial, temporal, or demographic variation. Implementation in Julia could enable composable modelling frameworks that integrate delay distributions with other epidemic components, building on interoperability concepts [@Nicholson2022-ua]. Such developments could make these methods part of a standard outbreak analysis toolbox, helping to address current barriers in settings where models could provide actionable insights for public health decision-making.

### Conclusions

Our primary event censored distribution approach addresses key challenges in epidemiological delay modelling by providing a statistically rigorous yet computationally efficient framework for handling double interval censored data. We maintain the statistical integrity of current best practice approaches whilst improving computational scalability through marginalisation. Our implementations in R and Stan enable straightforward integration with existing epidemiological workflows, and our analytical solutions for common distributions eliminate numerical integration in many scenarios. Results from simulated data and real-world Ebola case data demonstrate that our approach accurately recovers delay distributions across varying censoring scenarios whilst outperforming existing methods in computational efficiency. By establishing this framework and providing accessible software tools, we improve delay distribution estimation in disease surveillance and outbreak analysis, contributing to more accurate and timely public health decision-making.

## References

## Supporting information

### Detailed analytical solutions

#### General framework for analytical solutions
- Explanation of when analytical solutions are possible
- Role of partial expectation in deriving solutions

#### Solutions for exponentially tilted primary event times
- Complete derivation of equation for F_CP
- Limiting case for uniform distribution

#### Gamma distribution
- Full derivation of partial expectation
- Complete survival function and PMF formulations

#### Lognormal distribution
- Full derivation of partial expectation
- Complete survival function and PMF formulations

#### Weibull distribution
- Full derivation of partial expectation 
- Complete survival function and PMF formulations

### Mathematical details of the naive comparison model
- Derivation of the model used for comparison
- Explanation of the limitations and assumptions
- Implementation details


### Connection to other approaches
- Connection to Park et al.
- Connection to Reich et al. 
- Connection to Cori et al.

### Extended numerical validation results
- Detailed accuracy metrics across all distributions and sample sizes
- Additional visualizations

### Extended parameter recovery results

#### Stan-based estimation
- Full results across all sample sizes
- Detailed convergence diagnostics
- Additional parameter visualizations

#### fitdistrplus-based estimation
- Full results across all sample sizes
- Convergence and optimization details

#### Comparison with naive models
- Complete comparative analysis
- Additional error metrics

### Extended case study results
- Complete parameter visualizations
- Additional comparative outputs
